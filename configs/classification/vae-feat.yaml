data:
  type: encoded  # clean, raw, encoded
  root_path: ../data
  train_subjects: [1, 2, 3, 4]
  test_subjects: [5]

model: 
  name: Transformer
  task_name: classification
  num_classes: 4
  seq_len: 120
  pred_len: 0
  label_len: 0
  embed_type: 0
  top_k: 5
  enc_in: 45
  dec_in: 1
  c_out: 1
  d_model: 512
  n_heads: 8      # number of heads
  e_layers: 2     # encoder layers
  d_layers: 1     # decoder layers
  d_ff: 2048      # dimension of fcn
  factor: 1       # attention factor
  embed: timeF    # timeF, fixed or learned
  dropout: 0.5
  activation: gelu
  output_attention: False
  freq: h

train:
  epochs: 10
  batch_size: 32
  patience: 10
  lr: 0.001
  lradj: type1
  weight_decay: 0.001
  log_every: 100

log:
  project: fatigue
  group: classfication-learned
  tag: train4test1

