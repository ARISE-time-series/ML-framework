data:
  type: encoded  # clean, raw, encoded
  encode_dir: ae-236
  root_path: ../data/fatigue
  train_subjects: [1, 2, 3, 4]
  test_subjects: [5]
  scale: False
  embedding: None
  cols: ['Lactate', 'Na', 'K', 'Current (uAmps)', 'Temperature (Â°C)']

model: 
  name: Transformer
  task_name: classification
  num_classes: 4
  seq_len: 120
  pred_len: 0
  label_len: 0
  embed_type: 0
  top_k: 5
  enc_in: 53  # 32 + 16 + 5 = 53
  dec_in: 1
  c_out: 1
  d_model: 512
  n_heads: 8      # number of heads
  e_layers: 2     # encoder layers
  d_layers: 1     # decoder layers
  d_ff: 2048      # dimension of fcn
  factor: 1       # attention factor
  embed: timeF    # timeF, fixed or learned
  dropout: 0.5
  activation: gelu
  output_attention: False
  freq: s

train:
  epochs: 30
  batch_size: 32
  patience: 10
  lr: 0.001
  lradj: type1
  weight_decay: 0.0001
  log_every: 100

log:
  project: fatigue
  group: classfication-learned
  tag: train4test5-ae8

