data:
  type: encoded  # clean, raw, encoded
  encode_dir: ../data/manual
  root_path: ../data
  train_subjects: [6, 7, 8, 1, 2, 3, 4]
  train_acts: ['Stroop']
  test_subjects: [5, 9]
  test_acts: ['Stroop']
  scale: False
  embedding: None
  cols: ['Current (uAmps)', 'Temperature (Â°C)']
  subset: 1.0

model: 
  name: Transformer
  task_name: forecast
  num_classes: 4
  seq_len: 120
  pred_len: 120
  label_len: 8
  embed_type: 0
  top_k: 5
  enc_in: 10  # 64 + 16 + 5 = 85
  dec_in: 1
  c_out: 1
  d_model: 512
  n_heads: 8      # number of heads
  e_layers: 4     # encoder layers
  d_layers: 4     # decoder layers
  d_ff: 2048      # dimension of fcn
  factor: 1       # attention factor
  embed: timeF    # timeF, fixed or learned
  dropout: 0.08807094382400112
  activation: gelu
  output_attention: False
  freq: s

train:
  epochs: 20
  batch_size: 32
  patience: 5
  lr: 0.0001
  lradj: type1
  weight_decay: 0.0
  log_every: 100
  mixup: 0.4790050457354743
  bandwidth: 3.6631782216040825

